数据仓库搭建
1、构建数仓
-- 数据接入层，基础输出层，贴源层
CREATE DATABASE ods LOCATION '/daas/ods';
-- 明细数据层，事实表
CREATE DATABASE dwd LOCATION '/daas/dwd';
-- 一致性维表层
CREATE DATABASE dim LOCATION '/daas/dim';
-- 汇总层，宽表，轻度汇总
CREATE DATABASE dws LOCATION '/daas/dws';
-- 应用层
CREATE DATABASE ads LOCATION '/daas/ads';
2、导入数据到业务数据库中
-- 1、创建表
create table szt.szt_data(
    car_no varchar(255) comment '车牌号',
    card_no varchar(255) comment '卡号',
    close_date varchar(255) comment '结算日期',
    company_name varchar(255) comment '公司名称',
    conn_mark int comment '联程标记',
    deal_date varchar(255) comment '交易日期时间',
    deal_money DECIMAL(10,2) comment '交易金额',
    deal_type varchar(255) comment '交易类型',
    deal_value DECIMAL(10,2) comment '交易值',
    equ_no varchar(255) comment '设备编码',
    station varchar(255) comment '线路站点'
);

-- 2、导入数据
mysql -uroot -p123456 --local_infile
set global local_infile=1;
LOAD DATA LOCAL INFILE '/root/szt_data.txt' INTO TABLE szt_data FIELDS TERMINATED BY ',';


-- 3、用户信息表
create table szt.szt_user(
    id varchar(255) comment '身份证',
    name varchar(255) comment '姓名',
    card_no varchar(255) comment '卡号',
    age INT  comment '年龄',
    sex varchar(255) comment '性别'
);
LOAD DATA LOCAL INFILE '/root/szt_user.txt' INTO TABLE szt.szt_user FIELDS TERMINATED BY ',';
3、数据采集
从业务系统采集数据，将数据保存到数据仓库的ODS层
使用数据采集工具实现数据采集（DataX，Sqqop, FlinkX）
1、下载安装DataX
# 1、上传解压配置环境变量
tar -xvf datax.tar.gz 

# 配置环境变量
vim /etc/profile

export DATAX_HOME=/usr/local/soft/datax
export PATH=$PATH:$DATAX_HOME/bin

source /etc/profile
2、使用datax采集
1、刷卡主数据
1、编写DataX配置文件
vim szt_data.json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 2
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "123456",
                        "column": [
                            "car_no",
                            "card_no",
                            "close_date",
                            "company_name",
                            "conn_mark",
                            "deal_date",
                            "deal_money",
                            "deal_type",
                            "deal_value",
                            "equ_no",
                            "station"
                        ],
                        "where":"date_format(deal_date,'%Y%m%d') ='${dt}'",
                        "splitPk": "equ_no",
                        "connection": [
                            {
                                "table": [
                                    "szt_data"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://master:3306/szt"
                                ]
                            }
                        ]
                    }
                },
               "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://master:9000",
                        "fileType": "orc",
                        "path": "/daas/ods/ods_szt_data/dt=${dt}",
                        "fileName": "part",
                        "column": [
                            { "name": "car_no","type": "String"},
                            { "name": "card_no","type": "String"},
                            { "name": "close_date","type": "String"},
                            { "name": "company_name","type": "String"},
                            { "name": "conn_mark","type": "int"},
                            { "name": "deal_date","type": "String"},
                            { "name": "deal_money","type": "DOUBLE"},
                            { "name": "deal_type","type": "String"},
                            { "name": "deal_value","type": "DOUBLE"},
                            { "name": "equ_no","type": "String"},
                            { "name": "station","type": "String"}
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\u0001",
                        "compress":"NONE"
                    }
                }
            }
        ]
    }
}

2、在hive创建表
create EXTERNAL table ods.ods_szt_data(
    car_no String comment '车牌号',
    card_no String comment '卡号',
    close_date String comment '结算日期',
    company_name String comment '公司名称',
    conn_mark int comment '联程标记',
    deal_date String comment '交易日期时间',
    deal_money DECIMAL(10,2) comment '交易金额',
    deal_type String comment '交易类型',
    deal_value DECIMAL(10,2) comment '交易值',
    equ_no String comment '设备编码',
    station String comment '线路站点'
)partitioned by(dt STRING comment '天分区')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC
LOCATION '/daas/ods/ods_szt_data';
3、启动DataX采集数据
show partitions ods.ods_szt_data;
vim ods_szt_data.sh
#!/bin/bash

# 时间参数
dt=$1

# 删除分区目录
hdfs dfs -rm -r /daas/ods/ods_szt_data/dt=${dt}
# 创建分区目录
hdfs dfs -mkdir -p /daas/ods/ods_szt_data/dt=${dt}
# 增量数据采集
datax.py szt_data.json -p"-Ddt=${dt}"
# 增加分区
hive -e"alter table ods.ods_szt_data add if not exists partition(dt='${dt}');"
4、执行脚本
bash ods_szt_data.sh 20180901
5、查看结果
select * from ods.ods_szt_data where dt='20180901' limit 100;
2、用户信息表
1、编写datax配置文件
vim szt_user.json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 2
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "123456",
                        "column": [
                            "id",
                            "name",
                            "card_no",
                            "age",
                            "sex"
                        ],
                        "splitPk": "age",
                        "connection": [
                            {
                                "table": [
                                    "szt_user"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://master:3306/szt"
                                ]
                            }
                        ]
                    }
                },
               "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://master:9000",
                        "fileType": "orc",
                        "path": "/daas/ods/ods_szt_user/dt=${dt}",
                        "fileName": "part",
                        "column": [
                            { "name": "id","type": "String"},
                        { "name": "name","type": "String"},
                        { "name": "card_no","type": "String"},
                        { "name": "age","type": "INT"},
                        { "name": "sex","type": "String"}
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\u0001",
                        "compress":"NONE"
                    }
                }
            }
        ]
    }
}

2、在hive中创建表
create EXTERNAL table ods.ods_szt_user(
    id STRING comment '身份证',
    name STRING comment '姓名',
    card_no  STRING comment '卡号',
    age INT  comment '年龄',
    sex STRING comment '性别'
)partitioned by(dt STRING comment '天分区')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC;
3、编写脚本
vim ods_szt_user.sh
#!/bin/bash

# 时间参数
dt=$1

# 删除分区目录
hdfs dfs -rm -r /daas/ods/ods_szt_user/dt=${dt}
# 创建分区目录
hdfs dfs -mkdir -p /daas/ods/ods_szt_user/dt=${dt}
# 增量数据采集
datax.py szt_user.json -p"-Ddt=${dt}"
# 增加分区
hive -e"alter table ods.ods_szt_user add if not exists partition(dt='${dt}');"
4、执行脚本
bash ods_szt_user.sh 20180901
4、DWD层
1、地铁出入站表
● 建表
create EXTERNAL table dwd.dwd_fact_szt_in_out_detail(
    car_no String comment '车牌号',
    card_no String comment '卡号',
    close_date String comment '结算日期',
    company_name String comment '公司名称',
    conn_mark int comment '联程标记',
    deal_date String comment '交易日期时间',
    deal_money DECIMAL(10,2) comment '交易金额',
    deal_value DECIMAL(10,2) comment '交易值',
    equ_no String comment '设备编码',
    station String comment '线路站点'
)partitioned by(dt STRING comment '天分区',deal_type STRING comment '交易类型')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC
LOCATION '/daas/dwd/dwd_fact_szt_in_out_detail';
● 处理数据
-- 数据清洗
insert overwrite table dwd.dwd_fact_szt_in_out_detail partition(dt='20180831',deal_type)
select 
car_no,
card_no,
close_date,
company_name,
conn_mark,
deal_date,
deal_money,
deal_value,
equ_no,
station,
case when deal_type = '地铁入站' then 'I' else 'O' end as deal_type
from 
ods.ods_szt_data
where dt='20180831'
and deal_type != '巴士'
and date_format(deal_date,'HH:mm') > '06:14'
and date_format(deal_date,'HH:mm') < '23:59';
vim dwd_fact_szt_in_out_detail.sh
#!/bin/bash
dt=$1
# 使用hive -e执行sql脚本
hive -e"
insert overwrite table dwd.dwd_fact_szt_in_out_detail partition(dt='${dt}',deal_type)
select 
car_no,
card_no,
close_date,
company_name,
conn_mark,
deal_date,
deal_money,
deal_value,
equ_no,
station,
case when deal_type = '地铁入站' then 'I' else 'O' end as deal_type
from 
ods.ods_szt_data
where dt='${dt}'
and deal_type != '巴士'
and date_format(deal_date,'HH:mm') > '06:14'
and date_format(deal_date,'HH:mm') < '23:59';
"
● 执行脚本
bash dwd_fact_szt_in_out_detail.sh 20180901
-- 按而分区过滤，避免全表扫描
select * from dwd.dwd_fact_szt_in_out_detail where dt='20180901' and deal_type='O' limit 100;
2、地铁乘坐事实表
● 建表
create EXTERNAL table dwd.dwd_take_subway_detail(
    card_no String comment '用户编号',
    car_no String comment '车牌号',
    take_day String comment '乘坐日期',
    in_company_name String comment '进站线路',
    out_company_name String comment '出战线路',
    in_station String comment '进站站点',
    out_station String comment '出战站点',
    in_time String comment '进站时间',
    out_time String comment '出站时间',
    in_equ_no String comment '进站设备编码',
    out_equ_no String comment '出战设备编码',
    conn_mark INT comment '联程标记',
    deal_money DOUBLE comment '交易金额',
    take_time DOUBLE comment '用时'
)partitioned by(dt STRING comment '天分区')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC
LOCATION '/daas/dwd/dwd_take_subway_detail';
● 处理数据
insert overwrite table dwd.dwd_take_subway_detail partition(dt='20180901')
select 
    card_no,
    car_no,
    date_format(in_deal_date,'yyyy-MM-dd') as take_day,
    in_company_name,
    out_company_name,
    in_station,
    out_station,
    in_deal_date,
    out_deal_date,
    in_equ_no,
    out_equ_no,
    if(in_company_name=out_company_name,'1','0') as conn_mark,
    deal_money,
    round((unix_timestamp(out_deal_date)-unix_timestamp(in_deal_date)) /60,2) as take_time
from
(
    select 
        car_no,
        card_no,
        deal_date as out_deal_date,
        station as out_station,
        company_name as out_company_name,
        equ_no as out_equ_no,
        deal_type,
        deal_money,
        lag(deal_type,1) over(partition by card_no order by deal_date) lag_deal_type, -- 获取前一天数据的进场站类型
        lag(company_name,1) over(partition by card_no order by deal_date) in_company_name,
        lag(station,1) over(partition by card_no order by deal_date) in_station,
        lag(deal_date,1) over(partition by card_no order by deal_date) in_deal_date,
        lag(equ_no,1) over(partition by card_no order by deal_date) in_equ_no
    from 
        dwd.dwd_fact_szt_in_out_detail
    where 
        dt='20180901'
) as a
where lag_deal_type = "I" and deal_type="O" -- 同一次乘坐
and out_station!='""'
and in_station!='""';
5、DIM
1、用户维度表
● 在dim层建表建表
create EXTERNAL table dim.dim_szt_user(
    id STRING comment '身份证',
    name STRING comment '姓名',
    card_no  STRING comment '卡号',
    age INT  comment '年龄',
    sex STRING comment '性别'
)partitioned by(dt STRING comment '天分区')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC;
● 处理数据
insert overwrite table dim.dim_szt_user partition(dt='20180901')
select 
    md5(id) as id,
    md5(name) as name,
    card_no,
    age,
    sex
from 
ods.ods_szt_user
where dt='20180901';
6、DWS
1、地铁乘坐宽表
● 建表
create EXTERNAL table dws.dws_take_subway_detail(
    card_no String comment '用户编号',
    car_no String comment '车牌号',
    take_year String comment '年',
    take_month String comment '月',
    take_day String comment '乘坐日期',
    in_company_name String comment '进站线路',
    out_company_name String comment '出战线路',
    in_station String comment '进站站点',
    out_station String comment '出战站点',
    in_time String comment '进站时间',
    out_time String comment '出站时间',
    in_equ_no String comment '进站设备编码',
    out_equ_no String comment '出战设备编码',
    age INT comment '年龄',
    sex STRING comment '性别',
    conn_mark INT comment '联程标记',
    deal_money DOUBLE comment '交易金额',
    take_time DOUBLE comment '用时'
)partitioned by(dt STRING comment '天分区')
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
STORED AS ORC;
● 处理数据
-- map join: hive在进行表关联时，自动将小表加载到内存中进行关联，在map端进行关联，不产生reduce,可以提高关联效率
-- 将小表加载到内存中进行关联
-- 禁用mapjoin
set hive.auto.convert.join.noconditionaltask=False;

insert overwrite table dws.dws_take_subway_detail partition(dt='20180901')
select 
    a.card_no,
    a.car_no,
    year(a.take_day)as take_year,
    month(a.take_day)as take_month,
    a.take_day,
    a.in_company_name,
    a.out_company_name,
    a.in_station,
    a.out_station,
    a.in_time,
    a.out_time,
    a.in_equ_no,
    a.out_equ_no,
    b.age,
    b.sex,
    a.conn_mark,
    a.deal_money,
    a.take_time
from 
(
    select * from 
    dwd.dwd_take_subway_detail
    where dt='20180901'
) as a
join
(
    select * from 
    dim.dim_szt_user
    where dt='20180901'
) as b
on a.card_no=b.card_no;
2、将数据导入CK
1、在ck中创建表
create database dws;

create table dws.dws_take_subway_detail(
    card_no String comment '用户编号',
    car_no String comment '车牌号',
    take_year String comment '年',
    take_month String comment '月',
    take_day String comment '乘坐日期',
    in_company_name String comment '进站线路',
    out_company_name String comment '出战线路',
    in_station String comment '进站站点',
    out_station String comment '出战站点',
    in_time String comment '进站时间',
    out_time String comment '出站时间',
    in_equ_no String comment '进站设备编码',
    out_equ_no String comment '出战设备编码',
    age UInt32 comment '年龄',
    sex String comment '性别',
    conn_mark UInt32 comment '联程标记',
    deal_money Float32 comment '交易金额',
    take_time Float32 comment '用时'
)ENGINE = MergeTree()
order by(card_no);
2、编写datax配置文件
vim dws_take_subway_detail.json
{
    "job": {
        "setting": {
            "speed": {
                "channel": 3
            }
        },
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "path": "/daas/dws/dws_take_subway_detail/dt=20180901/*",
                        "defaultFS": "hdfs://master:9000",
                        "column": [
                            {
                                "index": 0,
                                "type": "String"
                            },
                            {
                                "index": 1,
                                "type": "String"
                            },
                            {
                                "index": 2,
                                "type": "String"
                            },
                            {
                                "index": 3,
                                "type": "String"
                            },
                            {
                                "index": 4,
                                "type": "String"
                            },
                            {
                                "index": 5,
                                "type": "String"
                            },
                            {
                                "index": 6,
                                "type": "String"
                            },
                            {
                                "index": 7,
                                "type": "String"
                            },
                            {
                                "index": 8,
                                "type": "String"
                            },
                            {
                                "index": 9,
                                "type": "String"
                            },
                            {
                                "index": 10,
                                "type": "String"
                            },
                            {
                                "index": 11,
                                "type": "String"
                            },
                            {
                                "index": 12,
                                "type": "String"
                            },
                            {
                                "index": 13,
                                "type": "LONG"
                            },
                            {
                                "index": 14,
                                "type": "STRING"
                            },
                            {
                                "index": 15,
                                "type": "LONG"
                            },
                            {
                                "index": 16,
                                "type": "DOUBLE"
                            },
                            {
                                "index": 17,
                                "type": "DOUBLE"
                            }
                        ],
                        "fileType": "orc",
                        "encoding": "UTF-8",
                        "fieldDelimiter": "\u0001"
                    }
                },
                "writer": {
                    "name": "clickhousewriter",
                    "parameter": {
                        "username": "default",
                        "password": "123456",
                        "column": [
                            "card_no",
                            "car_no",
                            "take_year",
                            "take_month",
                            "take_day",
                            "in_company_name",
                            "out_company_name",
                            "in_station",
                            "out_station",
                            "in_time",
                            "out_time",
                            "in_equ_no",
                            "out_equ_no",
                            "age",
                            "sex",
                            "conn_mark",
                            "deal_money",
                            "take_time"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:clickhouse://master:8123/dws",
                                "table": [
                                    "dws_take_subway_detail"
                                ]
                            }
                        ],
                        "preSql": [],
                        "postSql": [],
                        "batchSize": 65536,
                        "batchByteSize": 134217728,
                        "dryRun": false,
                        "writeMode": "insert"
                    }
                }
            }
        ]
    }
}
3、启动datax
datax.py dws_take_subway_detail.json
6、ADS
1、进站人次排行榜
select station,count(1) as flow from 
dwd.dwd_fact_szt_in_out_detail
where dt='20180901'
and deal_type='I'
and station != '""'
group by station
order by flow desc
limit 100;
2、每日运输乘客最多的车站区间排行榜
select 
case 
when in_station>out_station then concat(in_station,'-',out_station )
else concat(out_station,'-',in_station) end, 
count(1) as flow from (
    select 
        car_no,
        card_no,
        deal_date,
        station as out_station,
        deal_type,
        lag(deal_type,1) over(partition by card_no order by deal_date) lag_deal_type, -- 获取前一天数据的进场站类型
        lag(station,1) over(partition by card_no order by deal_date) in_station
    from 
    dwd.dwd_fact_szt_in_out_detail
    where dt='20180901'
    and station!='""'
) as a
where
deal_type = 'O' and lag_deal_type='I' -- 取出同一次乘坐
group by 
case 
when in_station>out_station then concat(in_station,'-',out_station )
else concat(out_station,'-',in_station) end
order by flow desc 
limit 100;


-- 基于乘坐表计算指标
select 
    case when in_station>out_station then concat(in_station,'-',out_station)
    else concat(out_station,'-',in_station) end ,
    count(1) as flow
from 
   dwd.dwd_take_subway_detail 
where 
    dt='20180901'
group by 
    case when in_station>out_station then concat(in_station,'-',out_station)
    else concat(out_station,'-',in_station) end 
order by 
    flow desc 
limit 100;


-- 基于宽表计算指标
select 
    case when in_station>out_station then concat(in_station,'-',out_station)
    else concat(out_station,'-',in_station) end ,
    count(1) as flow
from 
    dws.dws_take_subway_detail
where 
    dt='20180901'
group by 
    case when in_station>out_station then concat(in_station,'-',out_station)
    else concat(out_station,'-',in_station) end 
order by 
    flow desc 
limit 100;

-- 在ck中计算指标
select 
    case when in_station>out_station then concat(in_station,'-',out_station) 
    else concat(out_station,'-',in_station) end  as qj,
    count(1) as flow
from 
    dws.dws_take_subway_detail
group by 
    case when in_station>out_station then concat(in_station,'-',out_station)
    else concat(out_station,'-',in_station) end 
order by 
    flow desc 
limit 100;
3、线路单程直达乘客耗时平均值排行榜
select 
in_company_name,
round(avg((unix_timestamp(out_deal_date)-unix_timestamp(in_deal_date))/60),2) as avg_time
from (
   select 
        car_no,
        card_no,
        deal_date as out_deal_date,
        company_name as out_company_name,
        deal_type,
        lag(deal_type,1) over(partition by card_no order by deal_date) lag_deal_type, -- 获取前一天数据的进场站类型
        lag(company_name,1) over(partition by card_no order by deal_date) in_company_name,
        lag(deal_date,1) over(partition by card_no order by deal_date) in_deal_date
    from 
    dwd.dwd_fact_szt_in_out_detail
    where dt='20180901'
    and station!='""'
)as a
where
lag_deal_type = "I" and deal_type="O"
and in_company_name=out_company_name
group by in_company_name
order by avg_time desc;

-- 每天（统计周期）线路（粒度）单程直达（业务限定）乘客耗时（原子指标）平均值排行榜
select 
    in_company_name,
    round(avg(take_time),2) as avg_take_time -- 对原子指标进行计算
from 
    dwd.dwd_take_subway_detail 
where
    dt = '20180901' -- 统计周期
and
   conn_mark='1' -- 业务限定
group by
     -- 统计粒度
   in_company_name
order by 
   avg_take_time desc;

-- 每天（统计周期）线路（粒度）单程直达（业务限定）不同性别（粒度）乘客耗时（原子指标）平均值排行榜
-- 将结果保存到表中
create table ads.ads_line_single_ride_average_time_day_top as 
select 
    in_company_name,
    sex,
    round(avg(take_time),2) as avg_take_time -- 对原子指标进行计算
from 
    dws.dws_take_subway_detail
where
    dt = '20180901' -- 统计周期
and
   conn_mark='1' -- 业务限定
group by
     -- 统计粒度
   in_company_name,
   sex
order by 
   avg_take_time desc;

-- 不同周期，不同维度，不同粒度，不同原子指标的组合情况太多，如果每一个组合都要在hive中保存一个结果，那么ads层就会由很多的表

-- 每天（统计周期）线路（粒度）单程直达（业务限定）不同性别（粒度）乘客耗时（原子指标）平均值排行榜
-- 每年（统计周期）线路（粒度）单程直达（业务限定）不同性别（粒度）乘客耗时（原子指标）平均值排行榜
-- 每月（统计周期）线路（粒度）单程直达（业务限定）不同性别（粒度）乘客耗时（原子指标）平均值排行榜

-- 每月（统计周期）线路（粒度）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜
-- 每年（统计周期）线路（粒度）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜
-- 每天（统计周期）线路（粒度）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜

-- 每月（统计周期）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜
-- 每年（统计周期）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜
-- 每天（统计周期）单程直达（业务限定）不同年龄（粒度）乘客耗时（原子指标）平均值排行榜

select 
    in_company_name,
    sex,
    round(avg(take_time),2) as avg_take_time -- 对原子指标进行计算
from 
    dws.dws_take_subway_detail
where
   conn_mark='1' -- 业务限定
group by
     -- 统计粒度
   in_company_name,
   sex
order by 
   avg_take_time desc;

-- 可以将指标固定下来，定义成一个视图
create database ads;
create view ads.ads_line_single_ride_average_time_day_top as 
select 
    in_company_name,
    age,
    round(avg(take_time),2) as avg_take_time -- 对原子指标进行计算
from 
    dws.dws_take_subway_detail
where
   conn_mark='1' -- 业务限定
group by
     -- 统计粒度
   in_company_name,
   age
order by 
   avg_take_time desc;
