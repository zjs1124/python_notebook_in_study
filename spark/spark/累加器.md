```python
from pyspark.context import SparkContext
sc = SparkContext(master="local", appName="demo20_rdd")
students_rdd = sc.textFile("../../data/students.txt", 2)

# count算子需要再启动一个新的job计算数量
# print(students_rdd.count())

# 使用累加器可以依附某一个任务计算累加值

# 1、在driver端定义累加器
count_acc = sc.accumulator(0)

def map_fun(line:str):
    #2、在Executor端进行累加
    count_acc.add(1)
    clazz = line.split(",")[-1]
    return clazz

clazz_rdd = students_rdd.map(map_fun)
clazz_rdd.foreach(print)
# 3、在driver端获取累加结果
count = count_acc.value
print(f"count:{count}")
```
***使用原因***
count() 算子会执行一个task任务，数据太大时task会变多
普通变量则无法返回注册给driver段,每个副本会Executor单独执行自己的副本
而accumulator则是spark中共享的机制,可以将累加器返回注册给driver端,在driver端使用.value就可以调出其值。